---
title: "Chapter 3 Practice Problems"
output: html_document
---
``` {r, warning = FALSE}
library(faraway)
```


## 1. 
For the prostatedata, fit a model with lpsaas the response and the other vari-
ables as predictors:
``` {r}
data("prostate")
pro.fit <- lm(lpsa ~ ., data = prostate)
```

**(a) Compute 90 and 95% CIs for the parameter associated with age. Using just**
**these intervals, what could we have deduced about the p-value for age in the**
**regression summary?**
``` {r}
confint(pro.fit, level = .9)[4, ]
confint(pro.fit, level = .95)[4, ]
```

From the output, we can deduce that the age parameter is not statistically significant
at the the 95% confidence level, because the interval contains 0. However, it is 
at the 90% confidence level, because it doesn't contain 0.

**(b) Compute and display a 95% joint confidence region for the parameters asso-**
**ciated with age and lbph. Plot the origin on this display. The location of the**
**origin on the display tells us the outcome of a certain hypothesis test. State that**
**test and its outcome.**
``` {r, warning = FALSE}
library(ellipse)
plot(ellipse(pro.fit, c(4, 5)), type = "l")
points(0, 0)
```


This test corresponds to:

$$ H_0: \beta_{lbph} = \beta_{age} = 0 |\ \textrm{The other regressors are in the model} $$
And since the point is inside the ellipse, we must fail to reject the null hypothesis
and claim that the age and lbph regression coefficients could be 0. 


**(c) In the text, we made a permutation test corresponding to the F-test for the**
**significance of all the predictors. Execute the permutation test corresponding**
**to the t-test for age in this model. (Hint: {summary(g)$coef[4,3] gets you**
**the t-statistic you need if the model is called g.)**

``` {r} 
n <- 1000
obs <- summary(pro.fit)$coef[4, 3]
tstat <- vector("numeric", length = n)

for (i in 1:n) {
  mod <- lm(lpsa ~ lcavol + lweight + sample(age) + lbph + svi + lcp + gleason +  pgg45, data = prostate)
  tstat[i] <- summary(mod)$coef[4, 3]
}

mean(abs(tstat) > abs(obs))
```


**(d) Remove all the predictors that are not significant at the 5% level. Test this**
**model against the original model. Which model is preferred?**
```{r}
summary(pro.fit)
```

From the above output, we can see that the age, lbph, lcp, gleason, and pgg45 
regressors are not significant at the 5% level. 

```{r}
pro.lil <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
anova(pro.lil, pro.fit)
```

From the above test, we get an F-statistic of 1.4434, and a p-value or .2167. Thus,
we must fail to reject the null, in this case that the age, lbph, lcp, gleason, and
pgg45 regression coefficents are 0. Thus, these regressors do not need to be in our 
model.


### 3. 
Using the teengambdata, fit a model with gambleas the response and the other
variables as predictors.
``` {r}
data("teengamb")
gamb.fit <- lm(gamble ~ ., data = teengamb)
```

**(a) Which variables are statistically significant at the 5% level?**
``` {r}
summary(gamb.fit)
``` 

From the output, we can see that the sex and income variables are significant at 
the 5% level. 

**(b) What interpretation should be given to the coefficient for sex?**
With similar values for the other regressors, we would expect that on average a female 
will gamble $22.12 less each year than a male counterpart.

**(c) Fit a model with just incomeas a predictor and use an F-test to compare it to the full model.**
``` {r}
gamb.lil <- lm(gamble ~ income, data = teengamb)
anova(gamb.lil, gamb.fit)
``` 


From the output, we can see that our F-statistic is 4.1338 and our p-value is .01177. 
Our null hypothesis in this case is:

$$ H_0: \beta_{sex} = \beta_{status} = \beta_{verbal} = 0 | \textrm{The income regressor is included} $$

With the p-value received, we have strong evidence to reject the null hypothesis, 
and shold claim that at least one of the other regressors should be included in our
model. 

###4. 
Using the `sat` data
**(a) Fit a model with total sat score as the response and expend, ratio and**
**salary as predictors. Test the hypothesis that ?? salary = 0. Test the hypoth-**
**esis that ?? salary = ?? ratio = ?? expend = 0. Do any of these predictors have an**
**effect on the response?**
``` {r}
data("sat")
sat.fit <- lm(total ~ expend + ratio + salary, data = sat)
sat.lil <- lm(total ~ expend + ratio, data = sat)
anova(sat.lil, sat.fit)
```

From this output, we can see that the our F-statistic is 3.5285 and our p-value 
is .06667. Thus, we don't have enough evidence to reject the null hypothesis, 
and we should claim that the `salary` variable's regression coefficient could be 0. 

``` {r}
sat.null <- lm(total ~ 1, data = sat)
anova(sat.null, sat.fit)
```

Fromt the above test, we received a F-statistic of 4.0662 and a p-value of .01209.
Thus, we have moderate eveidence that we should reject the null hypothesis, 
and claim that these regressors do have some effect on the response. 

**(b) Now add takers to the model. Test the hypothesis that ?? takers = 0. Compare**
**this model to the previous one using an F-test. Demonstrate that the F-test and**
**t-test here are equivalent.**
``` {r}
sat.full <- lm(total ~ expend + ratio + salary + takers, data = sat)
anova(sat.fit, sat.full)
```

From this test, we get a F-statistic of 157.75 and a p-value of practically 0. 
We have thus have very strong evidence to reject the null hypothesis, and claim 
that the `takers` variable should be included in our model. 

``` {r}
t <- summary(sat.full)$coef[5, 1] / summary(sat.full)$coef[5, 2]
t^2 
```

From this, we can see that the t-test and the F-test result in the same statistic. 


### 5. 
**Find a formula relating R-squared and the F-test for the regression.**

We know that RSS is defined as:

$$ R^2 = 1 - \frac{RSS}{TSS} $$

And that F is defined as 

$$ F = \frac{(TSS- RSS) * (n - p)}{RSS * (p - 1)} $$ 

Through some algebraic manipulation, we can define \(R^2\) as

$$ R^2 = \frac{TSS - RSS}{TSS} $$

Thus, through substitution, we have: 

$$ R^2 = 1 - \frac{1}{(1 + F\ \frac{p - 1}{n - p})}$$ 
The practical interpretation of this relationship is that a bigger \(R^2\) leads 
to higher values of F. So, if \(R^2\) is large (meaning the model fits the data
well), then the corresponding F-statistic will be high, which means there is 
strong evidence to reject the null hypothesis and at least some of the coefficients
are non-zero. 

### 6. 
**Thirty-nine MBA students were asked about happiness and how this related to**
**their income and social life. The data are found in happy. Fit a regression model**
**with happy as the response and the other four variables as predictors.**
``` {r}
data("happy")
happy.fit <- lm(happy ~ money + sex + love + work, data = happy)
```

**(a) Which predictors were statistically significant at the 1% level?**
``` {r}
summary(happy.fit)
```

From the `summary()` output, we can see that the `love` predictor is the only 
statistically significant at the 1% level. 

**(b) Use the table function to produce a numerical summary of the response. What**
**assumption used to perform the t-tests seems questionable in light of this summary?**
``` {r}
table(happy$happy)
```

Since our data is clearly discrete, the normality assumption is dubious. 

**(c) Use the permutation procedure described in Section 3.3 to test the significance**
**of the money predictor.**
``` {r}
n <- 1000
obs <- summary(happy.fit)$coef[2, 3]
tstat <- vector("numeric", length = n)

for (i in 1:n) {
  mod <- lm(happy ~ sample(money) + sex + love + work, data = happy)
  tstat[i] <- summary(mod)$coef[2, 3]
}

mean(tstat > obs)
```

This test corresponds to a null hypothesis of

$$ H_0: \beta_{money}\ = 0 |\ \beta_{sex},\ \beta_{love},\ \beta_{work},\ \neq 0 $$ 
From the permutation test, we get a p-value of .028, which is strong evidence that
we should reject the null hypothesis and conclude that the `money` predictor should
be included our model. This result is in slight disagreement with the `summary()` 
output of the model, which is due to the fact the the `summary()` output uses the
t-test to determine statistical significance. Since the response is not normally 
distributed, the t-test is not valid. 

**(d) Plot a histgram of the permutation t-statistics. Make sure you use the the probability**
**rather than frequency version of the histogram.**
``` {r}
hist(tstat, freq = FALSE)
```


**(e) Overlay an appropriate t-density over the histogram. Hint: Use grid <-**
**seq(-3, 3, length = 300) to create a grid of values, then use the dt function**
**to compute the t-density on this grid and the lines function to superimpose**
**the result.**
``` {r}
seq(-3, 3, length = 300)
hist(dt(c(-3, 3), df = 34))
```

**(f) Use the bootstrap procedure from Section 3.6 to compute 90% and 95% confidence**
**intervals for bmoney. Does zero fall within these confidence intervals?**
**Are these results consistent with previous tests?**
``` {r} 
n <- 1000
coefmat <- matrix(NA, n, 5)
resids <- residuals(happy.fit)
preds <- fitted(happy.fit)

for (i in 1:n) {
  booty <- preds + sample(resids, replace  = TRUE)
  bmod <- update(happy.fit, booty ~ .)
  coefmat[i, ] <- coef(bmod)
}

quantile(coefmat[, 2], c(.025, .975))
quantile(coefmat[, 2], c(.05, .95))
```


###7. 
**In the punting data, we find the average distance punted and hang times of 10**
**punts of an American football as related to various measures of leg strength for**
**13 volunteers.**
``` {r}
data("punting")
```

**(a) Fit a regression model with Distance as the response and the right and left leg**
**strengths and flexibilities as predictors. Which predictors are significant at the**
**5% level?**
``` {r}
punt.fit <- lm(Distance ~ RStr + LStr + RFlex + LFlex, data = punting)
summary(punt.fit)
```

From the summary output, we can see that none of the regressors are significant 
at the 5% level. 

**(b) Use an F-test to determine whether collectively these four predictors have a**
**relationship to the response.**
``` {r}
punt.null <- lm(Distance ~ 1, data = punting)
anova(punt.null, punt.fit)
```

**(c) Relative to the model in (a), test whether the right and left leg strengths have**
**the same effect.**
``` {r}
punt.sub <- lm(Distance ~ I(RStr + LStr), RFlex + LFlex, data = punting)
anova(punt.sub, punt.fit)
```

**(d) Construct a 95% confidence region for (?? RStr ,?? LStr ). Explain how the test in**
**(c) relates to this region.**

**(e) Fit a model to test the hypothesis that it is total leg strength defined by adding**
**the right and left leg strengths that is sufficient to predict the response in com-**
**parison to using individual left and right leg strengths.**

**(f) Relative to the model in (a), test whether the right and left leg flexibilities have**
**the same effect.**

**(g) Test for left-right symmetry by performing the tests in (c) and (f) simultane-**
**ously.**

**(h) Fit a model with Hang as the response and the same four predictors. Can we**
**make a test to compare this model to that used in (a)? Explain.**
